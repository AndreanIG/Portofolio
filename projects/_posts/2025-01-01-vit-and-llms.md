---
layout: page
title: "CSER: Context-Sensitive Emotion Recognition Framework using Visual and Textual Features"
subtitle: "Building a multimodal framework that integrates Vision Transformers and Vision-Language Models for human emotion understanding."
permalink: /projects/cser-emotion-recognition/
description: "Reflection on developing the CSER framework ‚Äî a context-sensitive emotion recognition model combining Vision Transformers and Vision-Language Models, and the challenges faced during research and dataset curation."
---

This project was one of the hardest and most technically demanding research works I have ever completed.  
The paper, <strong>CSER: Context-Sensitive Emotion Recognition Framework using Visual and Textual Features</strong>, introduces a multimodal system that include both <strong>Vision Transformers (ViT)</strong> and <strong>Vision-Language Models (VLMs)</strong> , specifically LLaVA and BLIP to improve emotion recognition in complex, real-world settings.  
This research was written in collaboration with <strong>Dr. Thuseethan Selvarajah</strong>, <strong>Prof. Sutharshan Rajasegarar</strong>, and <strong>Prof. John Yearwood</strong> all from various different institute, and submitted to <em>IEEE Transactions on Multimedia</em>.

---

### üí° Motivation

Most emotion recognition models focus on a single modality such as facial expressions or voice.  
In contrast, human emotion is shaped by context which can be summarised from the surrounding scene, body posture, and situation.  
The goal of CSER was to make emotion recognition <strong>context-aware</strong>, closing in the gap between how AI systems interpret emotions and how humans perceive them.

---

### ‚öôÔ∏è Technical Summary

CSER combines three ViT-based visual learners for <strong>face</strong>, <strong>body</strong>, and <strong>scene</strong> with textual embeddings generated by <strong>LLaVA</strong> and <strong>BLIP</strong>.  
All modalities are projected into a shared latent space and fused using <strong>multi-head</strong> and <strong>self-attention</strong> mechanisms.  

**Results:**  
- Achieved <strong>mAP = 40.49%</strong> on EMOTIC  
- Achieved <strong>mAP = 54.28%</strong> on HECO  
- Outperformed existing state-of-the-art methods on both datasets  
- Showed major gains in subtle emotions such as <em>sadness</em>, <em>fear</em>, and <em>embarrassment</em>

---

### üß† The Hard Part

This project required months of experimentation, dataset preparation, and a lot of set backs.  

**Dataset scarcity:**  
Finding <strong>group or context-sensitive emotion datasets</strong> was nearly impossible. EMOTIC and HECO were the only usable ones as they are available and their authors can be contacted, but both had fragmented labels and inconsistent annotations that needed remapping.

**Data quality issues:**  
Many images contained occluded faces, low light, or incomplete subjects.  
To maintain uniform input shapes, I had to implement fallback logic that replaced missing faces with zero matrices which is a bit tedious but necessary step to keep the training pipeline stable.

**Hardware constraints:**  
All experiments initially ran on a <strong>GTX 1660 Ti</strong>, forcing constant trade-offs between batch size, learning rate, and runtime stability.  
Training large vision-language models pushed the limits of available memory, often causing crashes mid-run.  
To overcome this, I used <strong>external GPU resources provided by Deakin University‚Äôs research infrastructure and Hugging Face‚Äôs compute credits</strong>.  
Setting up the remote GPU environments took significant effort such as configuring CUDA, aligning PyTorch versions, and establishing secure connections, however it was essential to complete the experiments at full scale.

**VLM integration:**  
LLaVA and BLIP produced embeddings with incompatible dimensions and token structures.  
Aligning and projecting them into a shared feature space was one of the most complex implementation challenges.  

Every part from data cleaning to writing tested both patience and my problem-solving skill.

---

### üí≠ Reflection

Despite the obstacles, completing CSER taught me what true research persistence feels like.  
It was a lesson in problem-solving under constraint, working with incomplete datasets, limited hardware, and long iteration cycles, yet still producing a model that achieved measurable progress.  

Being the youngest member in the author list, I handled most of the experimental setup, ablation design, and manuscript drafting.  
This experience taught me to think beyond implementation, also to question design decisions, justify model behavior, and communicate results in a professional manner.

It reminded me that scientific progress is not just about accuracy, it‚Äôs about the discipline to keep iterating when nothing seems to work.

---

### üìë Citation

Ignasius, A., Selvarajah, T., Rajasegarar, S., & Yearwood, J. (2025).  
<em>CSER: Context-Sensitive Emotion Recognition Framework using Visual and Textual Features.</em>  
IEEE Transactions on Multimedia (Under Review).

---

### üè∑ Keywords

Emotion Recognition ¬∑ Multimodal Learning ¬∑ Vision Transformer ¬∑ Vision-Language Models ¬∑ Affective Computing  

---

<p><small><em>‚ÄúCSER reminded me that innovation often begins where comfort ends, when you keep refining ideas long after the models stop cooperating.‚Äù</em></small></p>