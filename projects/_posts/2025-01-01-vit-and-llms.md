---
layout: page
title: "CSER: Context-Sensitive Emotion Recognition Framework using Visual and Textual Features"
subtitle: "Building a multimodal framework that integrates Vision Transformers and Vision-Language Models for human emotion understanding."
permalink: /projects/cser-emotion-recognition/
description: "Reflection on developing the CSER framework ‚Äî a context-sensitive emotion recognition model combining Vision Transformers and Vision-Language Models, and the challenges faced during research and dataset curation."
---

<section class="lead" style="text-align: justify;">
This project was one of the hardest and most technically demanding research works I have ever completed.  
The paper, <strong>CSER: Context-Sensitive Emotion Recognition Framework using Visual and Textual Features</strong>, introduces a multimodal system that integrates <strong>Vision Transformers (ViT)</strong> and <strong>Vision-Language Models (VLMs)</strong> ‚Äî specifically LLaVA and BLIP ‚Äî to improve emotion recognition in complex, real-world settings.  
It was written in collaboration with <strong>Dr. Thuseethan Selvarajah</strong>, <strong>Prof. Sutharshan Rajasegarar</strong>, and <strong>Prof. John Yearwood</strong> from Deakin University, and submitted to <em>IEEE Transactions on Multimedia</em>.
</section>

---

### üí° Motivation

Most emotion recognition models focus on a single modality such as facial expressions or voice.  
In contrast, human emotion is shaped by context ‚Äî the surrounding scene, body posture, and situation.  
The goal of CSER was to make emotion recognition <strong>context-aware</strong>, bridging the gap between how AI systems interpret emotions and how humans perceive them.

---

### ‚öôÔ∏è Technical Summary

CSER combines three ViT-based visual learners for the <strong>face</strong>, <strong>body</strong>, and <strong>scene</strong> with textual embeddings generated by <strong>LLaVA</strong> and <strong>BLIP</strong>.  
All modalities are projected into a shared latent space and fused using <strong>multi-head</strong> and <strong>self-attention</strong> mechanisms.  

**Results:**  
- Achieved <strong>mAP = 40.49%</strong> on EMOTIC  
- Achieved <strong>mAP = 54.28%</strong> on HECO  
- Outperformed existing state-of-the-art methods on both datasets  
- Showed major gains in subtle emotions such as <em>sadness</em>, <em>fear</em>, and <em>embarrassment</em>

---

### üß† The Hard Part

This project required months of experimentation, dataset preparation, and repeated failures.  

**Dataset scarcity:**  
Finding <strong>group or context-sensitive emotion datasets</strong> was nearly impossible. EMOTIC and HECO were the only usable ones, but both had fragmented labels and inconsistent annotations that needed manual remapping.

**Data quality issues:**  
Many images contained occluded faces, low light, or incomplete subjects.  
To maintain uniform input shapes, I had to implement fallback logic that replaced missing faces with zero matrices ‚Äî a tedious but necessary step to keep the training pipeline stable.

**Hardware constraints:**  
All experiments initially ran on a <strong>GTX 1660 Ti</strong>, forcing constant trade-offs between batch size, learning rate, and runtime stability.  
Training large vision-language models pushed the limits of available memory, often causing crashes mid-run.  
To overcome this, I used <strong>external GPU resources provided by Deakin University‚Äôs research infrastructure and Hugging Face‚Äôs compute credits</strong>.  
Setting up the remote GPU environments took significant effort ‚Äî configuring CUDA, aligning PyTorch versions, and establishing secure connections ‚Äî but it was essential to complete the experiments at full scale.

**VLM integration:**  
LLaVA and BLIP produced embeddings with incompatible dimensions and token structures.  
Aligning and projecting them into a shared feature space was one of the most complex implementation challenges.

**Paper formatting:**  
Preparing the IEEE manuscript involved condensing large figures, tuning Grad-CAM++ visualizations, and formatting 17 pages of results under strict LaTeX templates.  

Every part ‚Äî from data cleaning to writing ‚Äî tested both patience and problem-solving endurance.

---

### üí≠ Reflection

Despite the obstacles, completing CSER taught me what true research persistence feels like.  
It was a lesson in problem-solving under constraint ‚Äî working with incomplete datasets, limited hardware, and long iteration cycles ‚Äî yet still producing a model that achieved measurable progress.  

Being the youngest member in the author list, I handled most of the experimental setup, ablation design, and manuscript drafting.  
This experience taught me to think beyond implementation ‚Äî to question design decisions, justify model behavior, and communicate results in a research-standard way.

It reminded me that scientific progress is not just about accuracy; it‚Äôs about the discipline to keep iterating when nothing seems to work.

---

### üìë Citation

Ignasius, A., Selvarajah, T., Rajasegarar, S., & Yearwood, J. (2025).  
<em>CSER: Context-Sensitive Emotion Recognition Framework using Visual and Textual Features.</em>  
IEEE Transactions on Multimedia (Under Review).

---

### üè∑ Keywords

Emotion Recognition ¬∑ Multimodal Learning ¬∑ Vision Transformer ¬∑ Vision-Language Models ¬∑ Affective Computing  

---

<p><small><em>Submitted October 2025 ¬∑ IEEE Transactions on Multimedia (Under Review)</em></small></p>
